{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/50, Reward: 32.52, Epsilon: 0.980\n",
      "Episode 2/50, Reward: 21.32, Epsilon: 0.960\n",
      "Episode 3/50, Reward: 16.96, Epsilon: 0.941\n",
      "Episode 4/50, Reward: -4.50, Epsilon: 0.921\n",
      "Episode 5/50, Reward: 92.08, Epsilon: 0.901\n",
      "Episode 6/50, Reward: 51.25, Epsilon: 0.881\n",
      "Episode 7/50, Reward: 2.92, Epsilon: 0.861\n",
      "Episode 8/50, Reward: 48.86, Epsilon: 0.842\n",
      "Episode 9/50, Reward: 2.17, Epsilon: 0.822\n",
      "Episode 10/50, Reward: 17.43, Epsilon: 0.802\n",
      "Episode 11/50, Reward: -69.11, Epsilon: 0.782\n",
      "Episode 12/50, Reward: -4.97, Epsilon: 0.762\n",
      "Episode 13/50, Reward: -50.35, Epsilon: 0.743\n",
      "Episode 14/50, Reward: -25.98, Epsilon: 0.723\n",
      "Episode 15/50, Reward: -24.96, Epsilon: 0.703\n",
      "Episode 16/50, Reward: -55.85, Epsilon: 0.683\n",
      "Episode 17/50, Reward: 30.28, Epsilon: 0.663\n",
      "Episode 18/50, Reward: -54.07, Epsilon: 0.644\n",
      "Episode 19/50, Reward: 8.82, Epsilon: 0.624\n",
      "Episode 20/50, Reward: -51.01, Epsilon: 0.604\n",
      "Episode 21/50, Reward: 41.60, Epsilon: 0.584\n",
      "Episode 22/50, Reward: 90.74, Epsilon: 0.564\n",
      "Episode 23/50, Reward: 15.82, Epsilon: 0.545\n",
      "Episode 24/50, Reward: 96.35, Epsilon: 0.525\n",
      "Episode 25/50, Reward: 101.87, Epsilon: 0.505\n",
      "Episode 26/50, Reward: 135.69, Epsilon: 0.485\n",
      "Episode 27/50, Reward: 60.57, Epsilon: 0.465\n",
      "Episode 28/50, Reward: 78.67, Epsilon: 0.446\n",
      "Episode 29/50, Reward: 56.33, Epsilon: 0.426\n",
      "Episode 30/50, Reward: 74.25, Epsilon: 0.406\n",
      "Episode 31/50, Reward: 70.92, Epsilon: 0.386\n",
      "Episode 32/50, Reward: 70.30, Epsilon: 0.366\n",
      "Episode 33/50, Reward: 119.06, Epsilon: 0.347\n",
      "Episode 34/50, Reward: 87.38, Epsilon: 0.327\n",
      "Episode 35/50, Reward: 27.61, Epsilon: 0.307\n",
      "Episode 36/50, Reward: 13.27, Epsilon: 0.287\n",
      "Episode 37/50, Reward: 98.73, Epsilon: 0.267\n",
      "Episode 38/50, Reward: 112.02, Epsilon: 0.248\n",
      "Episode 39/50, Reward: 106.66, Epsilon: 0.228\n",
      "Episode 40/50, Reward: 80.70, Epsilon: 0.208\n",
      "Episode 41/50, Reward: 100.79, Epsilon: 0.188\n",
      "Episode 42/50, Reward: 135.74, Epsilon: 0.168\n",
      "Episode 43/50, Reward: 81.97, Epsilon: 0.149\n",
      "Episode 44/50, Reward: 98.72, Epsilon: 0.129\n",
      "Episode 45/50, Reward: 89.54, Epsilon: 0.109\n",
      "Episode 46/50, Reward: 105.49, Epsilon: 0.089\n",
      "Episode 47/50, Reward: 104.48, Epsilon: 0.069\n",
      "Episode 48/50, Reward: 146.94, Epsilon: 0.050\n",
      "Episode 49/50, Reward: 123.10, Epsilon: 0.030\n",
      "Episode 50/50, Reward: 139.00, Epsilon: 0.010\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Test reward with the trained policy: 128.20\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \n",
    "    \"cuda\" if torch.cuda.is_available() else \n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "################################################################################\n",
    "# 1) A simple StockTrading environment (single-stock, discrete action).\n",
    "################################################################################\n",
    "class StockTradingEnv:\n",
    "    \"\"\"\n",
    "    A simplified environment for stock trading based on a single price time series.\n",
    "    \n",
    "    State:  A window of the most recent price data (e.g., last N prices).\n",
    "    Action: Discrete {0,1,2} corresponding to [SELL, HOLD, BUY].\n",
    "    Reward: Change in (unrealized/realized) PnL whenever we switch positions or close out.\n",
    "    \n",
    "    This example does not include transaction costs, slippage, or risk controls.\n",
    "    \"\"\"\n",
    "    def __init__(self, prices, window_size=30, initial_capital=10000, max_steps=1000):\n",
    "        \"\"\"\n",
    "        :param prices:        (np.ndarray) 1D array of daily (or t-step) prices.\n",
    "        :param window_size:   (int) number of past observations for state.\n",
    "        :param initial_capital: (float) initial capital for PnL calculations (not fully used here).\n",
    "        :param max_steps:     (int) maximum number of steps in an episode.\n",
    "        \"\"\"\n",
    "        self.prices = prices\n",
    "        self.window_size = window_size\n",
    "        self.initial_capital = initial_capital\n",
    "        self.max_steps = min(max_steps, len(prices) - window_size - 1)\n",
    "        \n",
    "        # Internal states\n",
    "        self.current_step = None\n",
    "        self.done = None\n",
    "        self.position = None  # +1 long, 0 flat, -1 short\n",
    "        self.capital = None\n",
    "        self.last_price = None\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to a starting state.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.position = 0\n",
    "        self.capital = self.initial_capital\n",
    "        self.last_price = self.prices[self.window_size - 1]\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current state: the last `window_size` prices.\n",
    "        Shape: (window_size,)\n",
    "        \"\"\"\n",
    "        start = self.current_step\n",
    "        end = self.current_step + self.window_size\n",
    "        window_prices = self.prices[start:end]\n",
    "        return window_prices\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes the chosen action.\n",
    "        :param action: int in {0, 1, 2}, mapped to [-1, 0, +1].\n",
    "        :return: (next_state, reward, done, info)\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0.0, True, {}\n",
    "\n",
    "        # Map discrete action into [-1, 0, +1]\n",
    "        if action == 0:\n",
    "            new_position = -1\n",
    "        elif action == 1:\n",
    "            new_position = 0\n",
    "        else:\n",
    "            new_position = 1\n",
    "\n",
    "        current_price = self.prices[self.current_step + self.window_size - 1]\n",
    "        reward = 0.0\n",
    "\n",
    "        # If we had a position, realize PnL from last_price to current_price\n",
    "        if self.position != 0:\n",
    "            reward += (current_price - self.last_price) * self.position\n",
    "\n",
    "        # Update position\n",
    "        self.position = new_position\n",
    "        self.last_price = current_price\n",
    "\n",
    "        # Move forward\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        # If done, close out any position at the final price\n",
    "        if self.done and self.position != 0:\n",
    "            final_price = self.prices[self.current_step + self.window_size - 1]\n",
    "            reward += (final_price - self.last_price) * self.position\n",
    "\n",
    "        # Prepare for next observation\n",
    "        next_state = self._get_observation()\n",
    "        return next_state, reward, self.done, {}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 2) CNN Q-Network (1D).\n",
    "################################################################################\n",
    "class CNNQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A 1D CNN that outputs Q-values for each of 3 actions: SELL, HOLD, BUY.\n",
    "    Input shape: (batch_size, 1, window_size)\n",
    "    Output shape: (batch_size, num_actions=3)\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size=30, num_actions=3):\n",
    "        super(CNNQNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, stride=1)\n",
    "        # After two conv layers (each kernel_size=5, stride=1):\n",
    "        # output length = window_size - 4 - 4 = window_size - 8\n",
    "        # So flatten dimension = 32*(window_size - 8)\n",
    "        self.fc1 = nn.Linear(32*(window_size - 8), 64)\n",
    "        self.fc2 = nn.Linear(64, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 1, window_size)\n",
    "        x = torch.relu(self.conv1(x))    # => (batch_size, 16, window_size-4)\n",
    "        x = torch.relu(self.conv2(x))    # => (batch_size, 32, window_size-8)\n",
    "        x = x.view(x.size(0), -1)        # flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)                  # => (batch_size, num_actions)\n",
    "        return x\n",
    "\n",
    "################################################################################\n",
    "# 3) Replay Buffer for DQN.\n",
    "################################################################################\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "################################################################################\n",
    "# 4) Utility functions for training.\n",
    "################################################################################\n",
    "def get_epsilon(it, max_it, min_epsilon=0.01, max_epsilon=1.0):\n",
    "    \"\"\"\n",
    "    Linearly decay epsilon from max_epsilon to min_epsilon over max_it iterations.\n",
    "    \"\"\"\n",
    "    slope = -(max_epsilon - min_epsilon) / max_it\n",
    "    epsilon = max(min_epsilon, max_epsilon + slope * it)\n",
    "    return epsilon\n",
    "\n",
    "def process_state_cnn(state):\n",
    "    \"\"\"\n",
    "    Convert the environment's observation (window_size,) into shape (1,1,window_size)\n",
    "    for the CNN forward pass.\n",
    "    \"\"\"\n",
    "    state_t = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)\n",
    "    return state_t\n",
    "\n",
    "################################################################################\n",
    "# 5) The DQN training loop.\n",
    "################################################################################\n",
    "def train_dqn(env, num_episodes=100, window_size=30, gamma=0.99,\n",
    "              lr=1e-3, batch_size=32, max_steps_per_episode=1000):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    q_net = CNNQNetwork(window_size=window_size, num_actions=3).to(device)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "    episode_rewards = []\n",
    "    \n",
    "    max_iterations = num_episodes * max_steps_per_episode\n",
    "    iteration = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            iteration += 1\n",
    "            epsilon = get_epsilon(iteration, max_iterations)\n",
    "            \n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice([0,1,2])  # SELL=0, HOLD=1, BUY=2\n",
    "            else:\n",
    "                s_t = process_state_cnn(state).to(device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = q_net(s_t)  # shape: (1, 3)\n",
    "                    action = q_values.argmax(dim=1).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Sample from replay and train\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to tensors\n",
    "                states_b_t = torch.cat([process_state_cnn(s) for s in states_b]).to(device) \n",
    "                actions_b_t = torch.LongTensor(actions_b).to(device)\n",
    "                rewards_b_t = torch.FloatTensor(rewards_b).to(device)\n",
    "                next_states_b_t = torch.cat([process_state_cnn(ns) for ns in next_states_b]).to(device)\n",
    "                dones_b_t = torch.FloatTensor(dones_b).to(device)\n",
    "                \n",
    "                # current Q(s, a)\n",
    "                q_values_b = q_net(states_b_t)  # (batch_size, 3)\n",
    "                q_values_chosen = q_values_b.gather(1, actions_b_t.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                # target Q = r + gamma * max_a' Q(s', a') if not done\n",
    "                with torch.no_grad():\n",
    "                    q_next = q_net(next_states_b_t)         # (batch_size, 3)\n",
    "                    q_next_max = q_next.max(dim=1)[0]       # (batch_size,)\n",
    "                    q_target = rewards_b_t + gamma * q_next_max * (1 - dones_b_t)\n",
    "                \n",
    "                loss = nn.MSELoss()(q_values_chosen, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Reward: {episode_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "        \n",
    "    return q_net, episode_rewards\n",
    "\n",
    "################################################################################\n",
    "# 6) Running a trained agent (for inference).\n",
    "################################################################################\n",
    "def run_trained_agent(env, q_net, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Run a trained agent (q_net) on env (no exploration).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        s_t = process_state_cnn(state).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(s_t)  # (1, 3)\n",
    "            action = q_values.argmax(dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "################################################################################\n",
    "# 7) Main - put it all together.\n",
    "################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Generate synthetic prices (Geometric Brownian Motion) as an example\n",
    "    def generate_synthetic_prices(T=2000, s0=100, mu=0.0005, sigma=0.01):\n",
    "        prices = [s0]\n",
    "        for t in range(1, T):\n",
    "            prices.append(\n",
    "                prices[-1] * math.exp((mu - 0.5*sigma**2) + sigma*random.gauss(0,1))\n",
    "            )\n",
    "        return np.array(prices, dtype=np.float32)\n",
    "\n",
    "    prices_array = generate_synthetic_prices(T=3000, s0=100)\n",
    "    \n",
    "    # 2) Create the environment\n",
    "    window_size = 30\n",
    "    env = StockTradingEnv(prices_array, window_size=window_size, initial_capital=10000, max_steps=1000)\n",
    "\n",
    "    # 3) Train DQN\n",
    "    trained_qnet, rewards_history = train_dqn(env,\n",
    "                                              num_episodes=50,\n",
    "                                              window_size=window_size,\n",
    "                                              gamma=0.99,\n",
    "                                              lr=1e-3,\n",
    "                                              batch_size=32,\n",
    "                                              max_steps_per_episode=1000)\n",
    "    print(\"\\nTraining complete!\\n\")\n",
    "\n",
    "    # 4) Test / run the trained agent\n",
    "    test_reward = run_trained_agent(env, trained_qnet)\n",
    "    print(f\"Test reward with the trained policy: {test_reward:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
