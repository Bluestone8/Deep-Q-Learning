{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Episode 1/5, Reward: -31.24, Eps: 0.802\n",
      "Episode 2/5, Reward: -17.67, Eps: 0.604\n",
      "Episode 3/5, Reward: 64.03, Eps: 0.406\n",
      "Episode 4/5, Reward: 15.50, Eps: 0.208\n",
      "Episode 5/5, Reward: 14.88, Eps: 0.010\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Test reward with trained Transformer policy: 114.73\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \n",
    "    \"cuda\" if torch.cuda.is_available() else \n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "################################################################################\n",
    "# 1) StockTrading environment\n",
    "################################################################################\n",
    "class StockTradingEnv:\n",
    "    def __init__(self, prices, window_size=30, initial_capital=10000, max_steps=1000):\n",
    "        self.prices = prices\n",
    "        self.window_size = window_size\n",
    "        self.initial_capital = initial_capital\n",
    "        self.max_steps = min(max_steps, len(prices) - window_size - 1)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.position = 0\n",
    "        self.capital = self.initial_capital\n",
    "        self.last_price = self.prices[self.window_size - 1]\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        start = self.current_step\n",
    "        end = self.current_step + self.window_size\n",
    "        return self.prices[start:end]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0.0, True, {}\n",
    "\n",
    "        new_position = -1 if action == 0 else (1 if action == 2 else 0)\n",
    "        current_price = self.prices[self.current_step + self.window_size - 1]\n",
    "        reward = 0.0\n",
    "\n",
    "        if self.position != 0:\n",
    "            reward += (current_price - self.last_price) * self.position\n",
    "\n",
    "        self.position = new_position\n",
    "        self.last_price = current_price\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "            if self.position != 0:\n",
    "                final_price = self.prices[self.current_step + self.window_size - 1]\n",
    "                reward += (final_price - self.last_price) * self.position\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n",
    "################################################################################\n",
    "# 2) Transformer-based Q-Network\n",
    "################################################################################\n",
    "class TransformerQNetwork(nn.Module):\n",
    "    def __init__(self, window_size=30, d_model=64, nhead=4, num_layers=2, num_actions=3, dropout=0.1):\n",
    "        super(TransformerQNetwork, self).__init__()\n",
    "        self.embedding = nn.Linear(1, d_model)\n",
    "        self.positional_encoding = self._generate_positional_encoding(window_size, d_model)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead, \n",
    "                dropout=dropout, \n",
    "                batch_first=True  # Enable batch_first for better performance\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, num_actions)\n",
    "\n",
    "    def _generate_positional_encoding(self, length, d_model):\n",
    "        position = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(length, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.positional_encoding.to(x.device)\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, -1, :]  # Take the output corresponding to the last time step\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 3) Replay Buffer\n",
    "################################################################################\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "################################################################################\n",
    "# 4) Utility Functions\n",
    "################################################################################\n",
    "def get_epsilon(it, max_it, min_epsilon=0.01, max_epsilon=1.0):\n",
    "    slope = -(max_epsilon - min_epsilon) / max_it\n",
    "    return max(min_epsilon, max_epsilon + slope * it)\n",
    "\n",
    "def process_state_transformer(state):\n",
    "    \"\"\"\n",
    "    Converts the environment's state (NumPy array) into a PyTorch tensor on the correct device.\n",
    "    \"\"\"\n",
    "    if isinstance(state, np.ndarray):  # Handle NumPy input\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "    return state.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 5) DQN Training Loop\n",
    "################################################################################\n",
    "def train_dqn(env, num_episodes=100, window_size=30, gamma=0.99,\n",
    "              lr=1e-4, batch_size=32, max_steps_per_episode=1000):\n",
    "    q_net = TransformerQNetwork(window_size=window_size, d_model=64, nhead=4, num_layers=2, num_actions=3).to(device)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "    episode_rewards = []\n",
    "    max_iterations = num_episodes * max_steps_per_episode\n",
    "    iteration = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()  # Initial state from the environment (NumPy array)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            iteration += 1\n",
    "            epsilon = get_epsilon(iteration, max_iterations)\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice([0, 1, 2])\n",
    "            else:\n",
    "                s_t = process_state_transformer(state)  # Convert state to tensor\n",
    "                with torch.no_grad():\n",
    "                    q_values = q_net(s_t)\n",
    "                    action = q_values.argmax(dim=1).item()\n",
    "\n",
    "            # Take an action in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update state and accumulate rewards\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Perform training if replay buffer has enough samples\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)\n",
    "                states_b_t = torch.cat([process_state_transformer(s) for s in states_b])\n",
    "                actions_b_t = torch.LongTensor(actions_b).to(device)\n",
    "                rewards_b_t = torch.FloatTensor(rewards_b).to(device)\n",
    "                next_states_b_t = torch.cat([process_state_transformer(ns) for ns in next_states_b])\n",
    "                dones_b_t = torch.FloatTensor(dones_b).to(device)\n",
    "\n",
    "                # Calculate Q-values and targets\n",
    "                q_values_b = q_net(states_b_t)\n",
    "                q_values_chosen = q_values_b.gather(1, actions_b_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    q_next = q_net(next_states_b_t)\n",
    "                    q_next_max = q_next.max(dim=1)[0]\n",
    "                    q_target = rewards_b_t + gamma * q_next_max * (1 - dones_b_t)\n",
    "\n",
    "                # Backpropagation\n",
    "                loss = nn.MSELoss()(q_values_chosen, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Reward: {episode_reward:.2f}, Eps: {epsilon:.3f}\")\n",
    "\n",
    "    return q_net, episode_rewards\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 6) Run Trained Agent\n",
    "################################################################################\n",
    "def run_trained_agent(env, q_net, max_steps=1000):\n",
    "    state = torch.FloatTensor(env.reset()).to(device)\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        s_t = process_state_transformer(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(s_t)\n",
    "            action = q_values.argmax(dim=1).item()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = torch.FloatTensor(next_state).to(device)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "################################################################################\n",
    "# 7) Main Function\n",
    "################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    def generate_synthetic_prices(T=3000, s0=100, mu=0.0005, sigma=0.01):\n",
    "        prices = [s0]\n",
    "        for t in range(1, T):\n",
    "            prices.append(prices[-1] * math.exp((mu - 0.5 * sigma**2) + sigma * random.gauss(0, 1)))\n",
    "        return np.array(prices, dtype=np.float32)\n",
    "\n",
    "    prices_array = generate_synthetic_prices(T=3000, s0=100)\n",
    "    window_size = 30\n",
    "    env = StockTradingEnv(prices_array, window_size=window_size, initial_capital=10000, max_steps=1000)\n",
    "\n",
    "    trained_qnet, rewards_history = train_dqn(env, num_episodes=5, window_size=window_size, gamma=0.99, lr=1e-3, batch_size=32, max_steps_per_episode=1000)\n",
    "    print(\"\\nTraining complete!\\n\")\n",
    "    test_reward = run_trained_agent(env, trained_qnet)\n",
    "    print(f\"Test reward with trained Transformer policy: {test_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
