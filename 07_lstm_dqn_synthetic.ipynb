{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Episode 1/50, Reward: -14.39, Eps: 0.980\n",
      "Episode 2/50, Reward: -15.27, Eps: 0.960\n",
      "Episode 3/50, Reward: -2.62, Eps: 0.941\n",
      "Episode 4/50, Reward: 28.90, Eps: 0.921\n",
      "Episode 5/50, Reward: 78.06, Eps: 0.901\n",
      "Episode 6/50, Reward: 27.74, Eps: 0.881\n",
      "Episode 7/50, Reward: 31.92, Eps: 0.861\n",
      "Episode 8/50, Reward: -31.73, Eps: 0.842\n",
      "Episode 9/50, Reward: -31.07, Eps: 0.822\n",
      "Episode 10/50, Reward: 33.55, Eps: 0.802\n",
      "Episode 11/50, Reward: -1.77, Eps: 0.782\n",
      "Episode 12/50, Reward: 46.01, Eps: 0.762\n",
      "Episode 13/50, Reward: -19.34, Eps: 0.743\n",
      "Episode 14/50, Reward: -11.05, Eps: 0.723\n",
      "Episode 15/50, Reward: -37.24, Eps: 0.703\n",
      "Episode 16/50, Reward: -3.09, Eps: 0.683\n",
      "Episode 17/50, Reward: 25.56, Eps: 0.663\n",
      "Episode 18/50, Reward: -16.61, Eps: 0.644\n",
      "Episode 19/50, Reward: -7.14, Eps: 0.624\n",
      "Episode 20/50, Reward: -14.82, Eps: 0.604\n",
      "Episode 21/50, Reward: -24.89, Eps: 0.584\n",
      "Episode 22/50, Reward: 22.75, Eps: 0.564\n",
      "Episode 23/50, Reward: -6.66, Eps: 0.545\n",
      "Episode 24/50, Reward: 20.56, Eps: 0.525\n",
      "Episode 25/50, Reward: -2.96, Eps: 0.505\n",
      "Episode 26/50, Reward: 0.91, Eps: 0.485\n",
      "Episode 27/50, Reward: -5.20, Eps: 0.465\n",
      "Episode 28/50, Reward: -7.97, Eps: 0.446\n",
      "Episode 29/50, Reward: -6.42, Eps: 0.426\n",
      "Episode 30/50, Reward: 51.52, Eps: 0.406\n",
      "Episode 31/50, Reward: 37.82, Eps: 0.386\n",
      "Episode 32/50, Reward: -8.47, Eps: 0.366\n",
      "Episode 33/50, Reward: 15.58, Eps: 0.347\n",
      "Episode 34/50, Reward: 12.21, Eps: 0.327\n",
      "Episode 35/50, Reward: 15.92, Eps: 0.307\n",
      "Episode 36/50, Reward: 8.21, Eps: 0.287\n",
      "Episode 37/50, Reward: 18.51, Eps: 0.267\n",
      "Episode 38/50, Reward: 4.83, Eps: 0.248\n",
      "Episode 39/50, Reward: -1.10, Eps: 0.228\n",
      "Episode 40/50, Reward: 15.53, Eps: 0.208\n",
      "Episode 41/50, Reward: -20.33, Eps: 0.188\n",
      "Episode 42/50, Reward: -2.81, Eps: 0.168\n",
      "Episode 43/50, Reward: -27.76, Eps: 0.149\n",
      "Episode 44/50, Reward: -11.09, Eps: 0.129\n",
      "Episode 45/50, Reward: -25.50, Eps: 0.109\n",
      "Episode 46/50, Reward: -19.64, Eps: 0.089\n",
      "Episode 47/50, Reward: -16.16, Eps: 0.069\n",
      "Episode 48/50, Reward: -10.02, Eps: 0.050\n",
      "Episode 49/50, Reward: -2.15, Eps: 0.030\n",
      "Episode 50/50, Reward: -19.78, Eps: 0.010\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Test reward with trained LSTM policy: 27.58\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \n",
    "    \"cuda\" if torch.cuda.is_available() else \n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "################################################################################\n",
    "# 1) StockTrading environment\n",
    "################################################################################\n",
    "class StockTradingEnv:\n",
    "    def __init__(self, prices, window_size=30, initial_capital=10000, max_steps=1000):\n",
    "        self.prices = prices\n",
    "        self.window_size = window_size\n",
    "        self.initial_capital = initial_capital\n",
    "        self.max_steps = min(max_steps, len(prices) - window_size - 1)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.position = 0\n",
    "        self.capital = self.initial_capital\n",
    "        self.last_price = self.prices[self.window_size - 1]\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        start = self.current_step\n",
    "        end = self.current_step + self.window_size\n",
    "        return self.prices[start:end]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0.0, True, {}\n",
    "\n",
    "        new_position = -1 if action == 0 else (1 if action == 2 else 0)\n",
    "        current_price = self.prices[self.current_step + self.window_size - 1]\n",
    "        reward = 0.0\n",
    "\n",
    "        if self.position != 0:\n",
    "            reward += (current_price - self.last_price) * self.position\n",
    "\n",
    "        self.position = new_position\n",
    "        self.last_price = current_price\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "            if self.position != 0:\n",
    "                final_price = self.prices[self.current_step + self.window_size - 1]\n",
    "                reward += (final_price - self.last_price) * self.position\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n",
    "################################################################################\n",
    "# 2) LSTM-Based Q-Network\n",
    "################################################################################\n",
    "class LSTMQNetwork(nn.Module):\n",
    "    def __init__(self, window_size=30, hidden_dim=64, num_layers=2, num_actions=3):\n",
    "        super(LSTMQNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.1)\n",
    "        self.fc = nn.Linear(hidden_dim, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        q_vals = self.fc(last_hidden)\n",
    "        return q_vals\n",
    "\n",
    "################################################################################\n",
    "# 3) Replay Buffer\n",
    "################################################################################\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "################################################################################\n",
    "# 4) Utility Functions\n",
    "################################################################################\n",
    "def get_epsilon(it, max_it, min_epsilon=0.01, max_epsilon=1.0):\n",
    "    slope = -(max_epsilon - min_epsilon) / max_it\n",
    "    return max(min_epsilon, max_epsilon + slope * it)\n",
    "\n",
    "def process_state(state):\n",
    "    if isinstance(state, np.ndarray):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "    return state.unsqueeze(0)\n",
    "\n",
    "################################################################################\n",
    "# 5) Training Loop\n",
    "################################################################################\n",
    "def train_dqn(env, num_episodes=100, window_size=30, gamma=0.99, lr=1e-3, batch_size=32, max_steps_per_episode=1000):\n",
    "    q_net = LSTMQNetwork(window_size=window_size, hidden_dim=64, num_layers=2, num_actions=3).to(device)\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=10000)\n",
    "    episode_rewards = []\n",
    "\n",
    "    max_iterations = num_episodes * max_steps_per_episode\n",
    "    iteration = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            iteration += 1\n",
    "            epsilon = get_epsilon(iteration, max_iterations)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice([0, 1, 2])\n",
    "            else:\n",
    "                s_t = process_state(state)\n",
    "                with torch.no_grad():\n",
    "                    q_values = q_net(s_t)\n",
    "                    action = q_values.argmax(dim=1).item()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)\n",
    "                states_b_t = torch.cat([process_state(s) for s in states_b])\n",
    "                actions_b_t = torch.LongTensor(actions_b).to(device)\n",
    "                rewards_b_t = torch.FloatTensor(rewards_b).to(device)\n",
    "                next_states_b_t = torch.cat([process_state(ns) for ns in next_states_b])\n",
    "                dones_b_t = torch.FloatTensor(dones_b).to(device)\n",
    "\n",
    "                q_values_b = q_net(states_b_t)\n",
    "                q_values_chosen = q_values_b.gather(1, actions_b_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    q_next = q_net(next_states_b_t)\n",
    "                    q_next_max = q_next.max(dim=1)[0]\n",
    "                    q_target = rewards_b_t + gamma * q_next_max * (1 - dones_b_t)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values_chosen, q_target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Reward: {episode_reward:.2f}, Eps: {epsilon:.3f}\")\n",
    "\n",
    "    return q_net, episode_rewards\n",
    "\n",
    "################################################################################\n",
    "# 6) Test Trained Agent\n",
    "################################################################################\n",
    "def run_trained_agent(env, q_net, max_steps=1000):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        s_t = process_state(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(s_t)\n",
    "            action = q_values.argmax(dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "################################################################################\n",
    "# 7) Main Function\n",
    "################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    def generate_synthetic_prices(T=3000, s0=100, mu=0.0005, sigma=0.01):\n",
    "        prices = [s0]\n",
    "        for t in range(1, T):\n",
    "            prices.append(prices[-1] * math.exp((mu - 0.5 * sigma**2) + sigma * random.gauss(0, 1)))\n",
    "        return np.array(prices, dtype=np.float32)\n",
    "\n",
    "    prices_array = generate_synthetic_prices(T=3000, s0=100)\n",
    "    window_size = 30\n",
    "    env = StockTradingEnv(prices_array, window_size=window_size, initial_capital=10000, max_steps=1000)\n",
    "\n",
    "    trained_qnet, rewards_history = train_dqn(env, num_episodes=50, window_size=window_size, gamma=0.99, lr=1e-4, batch_size=32, max_steps_per_episode=1000)\n",
    "    print(\"\\nTraining complete!\\n\")\n",
    "\n",
    "    test_reward = run_trained_agent(env, trained_qnet)\n",
    "    print(f\"Test reward with trained LSTM policy: {test_reward:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
