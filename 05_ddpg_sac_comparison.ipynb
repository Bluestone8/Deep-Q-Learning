{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "import requests\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Stable-Baselines3 for production-grade RL algorithms\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# -------------------------------\n",
    "# Data Fetching (Historical Prices)\n",
    "# -------------------------------\n",
    "def fetch_historical_data(quote=\"KRW\", target=\"ETH\", hours=5000):\n",
    "    \"\"\"\n",
    "    Fetches hourly candlestick data from an API.\n",
    "    (In production, you would use robust data ingestion from your vendor.)\n",
    "    \"\"\"\n",
    "    base_url = f\"https://api.bithumb.com/public/candlestick/{target}_{quote}/1h\"\n",
    "    try:\n",
    "        response = requests.get(base_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get(\"status\") != \"0000\":\n",
    "            print(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
    "            return None\n",
    "            \n",
    "        candlesticks = data.get(\"data\", [])\n",
    "        if not candlesticks:\n",
    "            print(\"No hourly data found in response.\")\n",
    "            return None\n",
    "            \n",
    "        prices = [float(entry[2]) for entry in candlesticks[-hours:]]\n",
    "        return np.array(prices, dtype=np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching hourly data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------------\n",
    "# Custom Trading Environment\n",
    "# -------------------------------\n",
    "class TradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom trading environment for production–grade RL.\n",
    "    \n",
    "    Key Features:\n",
    "      - Continuous Action Space:\n",
    "          Action ∈ [-1, 1]: \n",
    "            >0: Buy (fraction of maximum affordable quantity)\n",
    "            <0: Sell (fraction of current position)\n",
    "            0:  Hold\n",
    "      - Observation: A window of normalized prices plus additional features:\n",
    "          * normalized balance\n",
    "          * current position\n",
    "          * volatility (std. of returns in the window)\n",
    "      - Reward: Log return on net worth at each step with a long-term bonus (and risk penalty)\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self, prices, window_size=30, initial_balance=1e6, longterm_weight=0.1, max_drawdown=0.3):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.prices = prices\n",
    "        self.window_size = window_size\n",
    "        self.initial_balance = initial_balance\n",
    "        self.longterm_weight = longterm_weight  # bonus weight at the end of the episode\n",
    "        self.max_drawdown = max_drawdown        # risk constraint: maximum allowed drawdown (as fraction)\n",
    "        \n",
    "        # Continuous action: one number in [-1, 1]\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        # Observation: window_size normalized prices + 3 extra features:\n",
    "        # [window_size prices, normalized balance, current position, volatility]\n",
    "        obs_low = -np.inf * np.ones(window_size + 3, dtype=np.float32)\n",
    "        obs_high = np.inf * np.ones(window_size + 3, dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0.0  # asset units held\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.max_net_worth = self.initial_balance  # track peak net worth for drawdown\n",
    "        self.trades = []  # log trades for evaluation\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        # Use the last window_size prices\n",
    "        window = self.prices[self.current_step - self.window_size:self.current_step]\n",
    "        mean_p = np.mean(window)\n",
    "        std_p = np.std(window) if np.std(window) > 0 else 1.0\n",
    "        normalized_prices = (window - mean_p) / std_p\n",
    "        \n",
    "        # Additional features:\n",
    "        norm_balance = self.balance / self.initial_balance\n",
    "        current_position = self.position  # could be normalized by a position limit\n",
    "        # Estimate volatility from returns in the window\n",
    "        returns = np.diff(window) / window[:-1] if len(window) > 1 else [0.0]\n",
    "        volatility = np.std(returns) if len(returns) > 1 else 0.0\n",
    "        \n",
    "        obs = np.concatenate([normalized_prices, [norm_balance, current_position, volatility]])\n",
    "        return obs.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one timestep of trading.\n",
    "        Action is continuous in [-1, 1].  \n",
    "          If action > 0: Buy fraction of maximum affordable units.\n",
    "          If action < 0: Sell fraction of current position.\n",
    "        \"\"\"\n",
    "        action_val = np.clip(action[0], -1.0, 1.0)\n",
    "        current_price = self.prices[self.current_step]\n",
    "        prev_net_worth = self.net_worth\n",
    "        \n",
    "        # Execute action:\n",
    "        if action_val > 0:  # BUY\n",
    "            # Determine maximum units affordable\n",
    "            max_units = self.balance / current_price\n",
    "            # Buy a fraction equal to action_val (e.g., 0.5 means spend 50% of cash)\n",
    "            units_bought = action_val * max_units\n",
    "            self.position += units_bought\n",
    "            self.balance -= units_bought * current_price\n",
    "            self.trades.append({\"step\": self.current_step, \"type\": \"buy\", \"units\": units_bought, \"price\": current_price})\n",
    "        elif action_val < 0:  # SELL\n",
    "            # Sell a fraction of current holdings\n",
    "            units_sold = abs(action_val) * self.position\n",
    "            self.position -= units_sold\n",
    "            self.balance += units_sold * current_price\n",
    "            self.trades.append({\"step\": self.current_step, \"type\": \"sell\", \"units\": units_sold, \"price\": current_price})\n",
    "        # Else, action_val == 0, so HOLD\n",
    "        \n",
    "        # Update net worth and track peak value (for drawdown risk management)\n",
    "        self.net_worth = self.balance + self.position * current_price\n",
    "        self.max_net_worth = max(self.max_net_worth, self.net_worth)\n",
    "        drawdown = (self.max_net_worth - self.net_worth) / self.max_net_worth\n",
    "        \n",
    "        # Reward: log return (if possible), penalized by excessive drawdown\n",
    "        reward = 0.0\n",
    "        if prev_net_worth > 0:\n",
    "            reward = np.log(self.net_worth / prev_net_worth)\n",
    "        \n",
    "        # Apply risk penalty if drawdown exceeds threshold\n",
    "        if drawdown > self.max_drawdown:\n",
    "            reward -= 1.0 * (drawdown - self.max_drawdown)  # risk penalty\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= len(self.prices) - 1)\n",
    "        \n",
    "        # At episode end, add a bonus based on overall profit (long-term PnL)\n",
    "        if done:\n",
    "            overall_profit = (self.net_worth - self.initial_balance) / self.initial_balance\n",
    "            reward += self.longterm_weight * overall_profit\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        info = {\"net_worth\": self.net_worth, \"balance\": self.balance, \"position\": self.position, \"drawdown\": drawdown}\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"Step: {self.current_step} | Net Worth: {self.net_worth:.2f} | Balance: {self.balance:.2f} | Position: {self.position:.4f}\")\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# -------------------------------\n",
    "# (Optional) Callback for Monitoring Training Progress\n",
    "# -------------------------------\n",
    "class NetWorthCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that logs the net worth at the end of each episode.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(NetWorthCallback, self).__init__(verbose)\n",
    "        self.episode_net_worths = []\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # You can extend this callback to log extra metrics\n",
    "        pass\n",
    "    \n",
    "    def _on_episode_end(self):\n",
    "        # Access the environment info (assumes Monitor wrapper is used)\n",
    "        if \"net_worth\" in self.locals.get(\"infos\", [{}])[-1]:\n",
    "            nw = self.locals[\"infos\"][-1][\"net_worth\"]\n",
    "            self.episode_net_worths.append(nw)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Episode finished with net worth: {nw:.2f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Main: Training and Evaluation Pipeline\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Data Management: Fetch historical data\n",
    "    coin = \"ETH\"\n",
    "    quote = \"KRW\"\n",
    "    total_hours = 2000  # adjust as needed (production systems will use much larger datasets)\n",
    "    prices = fetch_historical_data(quote=quote, target=coin, hours=total_hours)\n",
    "    if prices is None or len(prices) < total_hours:\n",
    "        print(\"Insufficient price data. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Optionally, split data into training and test sets.\n",
    "    # Here, we use the first 75% for training and the remaining 25% for evaluation.\n",
    "    split_idx = int(0.75 * len(prices))\n",
    "    train_prices = prices[:split_idx]\n",
    "    test_prices  = prices[split_idx:]\n",
    "    \n",
    "    # 2. Create the Gym environment (wrap with Monitor for logging)\n",
    "    window_size = 30\n",
    "    initial_balance = 1e6  # for example, 1,000,000 KRW\n",
    "    env = TradingEnv(train_prices, window_size=window_size, initial_balance=initial_balance)\n",
    "    env = Monitor(env)  # wrap for logging and evaluation\n",
    "    \n",
    "    # 3. Offline Pre-training or Imitation Learning could be added here\n",
    "    # For brevity, we proceed directly to RL fine-tuning.\n",
    "    \n",
    "    # 4. On-line Fine-Tuning with a production–grade RL algorithm (SAC in this example)\n",
    "    model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./sac_trading_tensorboard/\")\n",
    "    \n",
    "    # Total timesteps should be chosen based on data size and experimentation.\n",
    "    total_timesteps = 100_000\n",
    "    networth_callback = NetWorthCallback(verbose=1)\n",
    "    model.learn(total_timesteps=total_timesteps, callback=networth_callback)\n",
    "    \n",
    "    # Save the trained model for production deployment.\n",
    "    model_path = \"sac_trading_model\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}.zip\")\n",
    "    \n",
    "    # 5. Evaluation & Monitoring on Test Data\n",
    "    eval_env = TradingEnv(test_prices, window_size=window_size, initial_balance=initial_balance)\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    networth_history = [eval_env.net_worth]\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = eval_env.step(action)\n",
    "        networth_history.append(eval_env.net_worth)\n",
    "    \n",
    "    print(f\"Final net worth on evaluation data: {eval_env.net_worth:.2f}\")\n",
    "    \n",
    "    # 6. Plot the net worth over the evaluation episode\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(networth_history, label=\"Net Worth\")\n",
    "    plt.title(\"Evaluation: Net Worth Over Time\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Net Worth\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # In production, further steps would include:\n",
    "    #   - Integrating real-time data feeds.\n",
    "    #   - Risk management overlays (e.g., position limits, max drawdown stops).\n",
    "    #   - Order execution logic (e.g., VWAP slicing, limit orders).\n",
    "    #   - Extensive backtesting across multiple market regimes.\n",
    "    #   - Real-time performance dashboards and alerts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
